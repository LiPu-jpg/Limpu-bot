import os
from typing import Any, Optional, cast

from langchain_community.document_loaders import DirectoryLoader, TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from .config import config


class RagEngine:
    def __init__(self):
        self.vector_db: Optional[Chroma] = None
        self.retriever = None
        self.llm: Optional[ChatOpenAI] = None
        self.embeddings: Optional[HuggingFaceEmbeddings] = None

        # å»¶è¿Ÿåˆå§‹åŒ–ï¼šé¿å… bot å¯åŠ¨æ—¶å°±åŠ è½½ embedding/LLM
        # åªæœ‰åœ¨ /é—® æˆ– /é‡æ„çŸ¥è¯†åº“ æ—¶æ‰åˆå§‹åŒ–ã€‚

    def _ensure_initialized(self) -> None:
        if config.HF_ENDPOINT:
            os.environ["HF_ENDPOINT"] = config.HF_ENDPOINT

        if self.llm is None:
            if not config.AI_API_KEY:
                raise RuntimeError("æœªé…ç½® HITSZ_MANAGER_AI_API_KEY")

            # langchain_openai ä¸åŒç‰ˆæœ¬å‚æ•°åä¸ä¸€è‡´ï¼ŒåŠ¨æ€æ˜ å°„é¿å…è¿è¡Œæ—¶æŠ¥é”™
            fields = getattr(ChatOpenAI, "model_fields", {}) or {}
            # è¿™é‡Œç”¨ dict + **params æ˜¯ä¸ºäº†å…¼å®¹ä¸åŒç‰ˆæœ¬çš„ ChatOpenAI å‚æ•°åã€‚
            # ä½†è‹¥ä¸æ˜¾å¼æ ‡æ³¨ç±»å‹ï¼ŒPylance ä¼šæŠŠ params æ¨æ–­æˆ dict[str, float]ï¼Œ
            # ä»è€Œåœ¨åç»­å†™å…¥ str/bool ç­‰å€¼æ—¶æŠ¥ä¸€å †è¯¯æŠ¥ã€‚
            params: dict[str, Any] = {"temperature": 0.3}

            if "openai_api_key" in fields:
                params["openai_api_key"] = config.AI_API_KEY
            elif "api_key" in fields:
                params["api_key"] = config.AI_API_KEY

            if config.AI_BASE_URL:
                if "openai_api_base" in fields:
                    params["openai_api_base"] = config.AI_BASE_URL
                elif "base_url" in fields:
                    params["base_url"] = config.AI_BASE_URL

            if "model" in fields:
                params["model"] = config.AI_MODEL
            elif "model_name" in fields:
                params["model_name"] = config.AI_MODEL

            self.llm = ChatOpenAI(**cast(Any, params))

        if self.embeddings is None:
            # ä¸ºäº†é¿å…æ¯æ¬¡å®¹å™¨é‡å¯éƒ½é‡æ–°ä¸‹è½½æ¨¡å‹ï¼Œè¿™é‡ŒæŠŠç¼“å­˜å›ºå®šåˆ° data ç›®å½•ä¸‹ã€‚
            # è‹¥ data æŒ‚è½½ä¸º volumeï¼Œåˆ™æ¨¡å‹ä¸‹è½½ä¸€æ¬¡åå¯å¤ç”¨ã€‚
            cache_root = str((config.DATA_ROOT / "hf_cache").resolve())
            os.environ.setdefault("HF_HOME", cache_root)
            os.environ.setdefault("HUGGINGFACE_HUB_CACHE", os.path.join(cache_root, "hub"))
            os.environ.setdefault("TRANSFORMERS_CACHE", os.path.join(cache_root, "transformers"))

            print("ğŸ§  æ­£åœ¨åŠ è½½ Embedding æ¨¡å‹ (CPU)...")
            try:
                self.embeddings = HuggingFaceEmbeddings(
                    model_name=config.EMBEDDING_MODEL,
                    cache_folder=cache_root,
                    model_kwargs={"device": "cpu"},
                    encode_kwargs={"normalize_embeddings": True},
                )
            except Exception as e:
                hint = (
                    "Embedding æ¨¡å‹ä¸‹è½½/åŠ è½½å¤±è´¥ã€‚é€šå¸¸æ˜¯æœåŠ¡å™¨æ— æ³•è®¿é—® huggingface.coï¼Œä¸”æœ¬åœ°ç¼“å­˜ä¸å­˜åœ¨ã€‚\n"
                    f"- å½“å‰æ¨¡å‹ï¼š{config.EMBEDDING_MODEL}\n"
                    f"- ç¼“å­˜ç›®å½•ï¼š{cache_root}\n"
                    "å¯é€‰ä¿®å¤ï¼š\n"
                    "1) è®¾ç½® HuggingFace é•œåƒï¼šHITSZ_MANAGER_HF_ENDPOINT=https://hf-mirror.com ï¼ˆæˆ–ä½ å¯ç”¨çš„é•œåƒï¼‰å¹¶é‡å¯ï¼›\n"
                    "2) æˆ–åœ¨æœ‰ç½‘ç»œçš„æœºå™¨ä¸Šé¢„ä¸‹è½½è¯¥æ¨¡å‹åˆ°ä¸Šè¿°ç¼“å­˜ç›®å½•ï¼Œå†æ‹·è´/æŒ‚è½½åˆ°æœåŠ¡å™¨ã€‚"
                )
                raise RuntimeError(hint) from e

        self._load_existing_db()

    def _load_existing_db(self) -> None:
        if self.embeddings is None:
            return
        if config.VECTOR_DB_DIR.exists() and any(config.VECTOR_DB_DIR.iterdir()):
            try:
                self.vector_db = Chroma(
                    persist_directory=str(config.VECTOR_DB_DIR), 
                    embedding_function=self.embeddings
                )
                self.retriever = self.vector_db.as_retriever(search_kwargs={"k": 3})
                print("ğŸ“š æœ¬åœ°å‘é‡çŸ¥è¯†åº“åŠ è½½æˆåŠŸ")
            except Exception as e:
                print(f"âš ï¸ å‘é‡åº“åŠ è½½å¤±è´¥ (å¯èƒ½æ˜¯é¦–æ¬¡è¿è¡Œ): {e}")

    async def rebuild_index(self) -> str:
        """é‡å»ºçŸ¥è¯†åº“ç´¢å¼• (è€—æ—¶æ“ä½œ)"""
        try:
            self._ensure_initialized()
        except Exception as e:
            return f"âŒ åˆå§‹åŒ–å¤±è´¥: {e}"

        if not config.RAG_DOCS_DIR.exists():
            return "âŒ ç›®å½• data/rag_docs ä¸å­˜åœ¨"
        
        # 1. è¯»å–æ–‡ä»¶
        loader = DirectoryLoader(str(config.RAG_DOCS_DIR), glob="**/*.txt", loader_cls=TextLoader)
        docs = loader.load()
        if not docs:
            return "âš ï¸ data/rag_docs ç›®å½•ä¸‹æ²¡æœ‰ .txt æ–‡ä»¶"

        # 2. åˆ‡åˆ†æ–‡æœ¬
        splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)
        splits = splitter.split_documents(docs)

        # 3. å†™å…¥ Chroma
        # æ³¨æ„ï¼šè¿™é‡Œä¼šé‡æ–°ç”Ÿæˆæ•´ä¸ªåº“
        try:
            if self.vector_db:
                # å°è¯•æ¸…ç†æ—§æ•°æ®ï¼Œæˆ–è€…ç›´æ¥è¦†ç›–ç›®å½•
                self.vector_db = None
            
            # åˆ›å»ºæ–°çš„ DB
            self.vector_db = Chroma.from_documents(
                documents=splits,
                embedding=self.embeddings,
                persist_directory=str(config.VECTOR_DB_DIR)
            )
            self.retriever = self.vector_db.as_retriever(search_kwargs={"k": 3})
            return f"âœ… çŸ¥è¯†åº“æ„å»ºå®Œæˆï¼å…±ç´¢å¼• {len(splits)} ä¸ªæ–‡æœ¬ç‰‡æ®µã€‚"
        except Exception as e:
            return f"âŒ æ„å»ºå¤±è´¥: {e}"

    async def query(self, question: str) -> str:
        """RAG é—®ç­”æµç¨‹"""
        try:
            self._ensure_initialized()
        except Exception as e:
            return f"âŒ åˆå§‹åŒ–å¤±è´¥: {e}"

        if not self.retriever:
            return "âš ï¸ çŸ¥è¯†åº“å°šæœªåˆå§‹åŒ–ï¼Œè¯·å…ˆä½¿ç”¨æŒ‡ä»¤æ„å»ºçŸ¥è¯†åº“ã€‚"

        llm = self.llm
        if llm is None:
            return "âŒ LLM æœªåˆå§‹åŒ–"

        template = """ä½ æ˜¯ä¸€ä¸ªå“ˆå·¥å¤§æ·±åœ³(HITSZ)çš„æ ¡å›­åŠ©æ‰‹ã€‚è¯·æ ¹æ®ä»¥ä¸‹å·²çŸ¥ä¿¡æ¯å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚
        
        ä¸¥æ ¼éµå®ˆä»¥ä¸‹è§„åˆ™ï¼š
        1. ä»…æ ¹æ®[å·²çŸ¥ä¿¡æ¯]å›ç­”ï¼Œä¸è¦ç¼–é€ å†…å®¹ã€‚
        2. å¦‚æœå·²çŸ¥ä¿¡æ¯ä¸­æ²¡æœ‰ç­”æ¡ˆï¼Œè¯·ç›´æ¥å›ç­”â€œæŠ±æ­‰ï¼ŒçŸ¥è¯†åº“ä¸­æš‚æ—¶æ²¡æœ‰ç›¸å…³ä¿¡æ¯â€ã€‚
        3. å›ç­”è¦ç®€æ´æ˜äº†ã€‚

        [å·²çŸ¥ä¿¡æ¯]:
        {context}

        [ç”¨æˆ·é—®é¢˜]: {question}
        """
        prompt = ChatPromptTemplate.from_template(template)

        chain = (
            {"context": self.retriever, "question": RunnablePassthrough()}
            | prompt
            | llm
            | StrOutputParser()
        )

        try:
            return await chain.ainvoke(question)
        except Exception as e:
            return f"âŒ AI å‘ç”Ÿé”™è¯¯: {e}"

# å…¨å±€å•ä¾‹
rag_engine = RagEngine()
