import os
from typing import Optional

from langchain_community.document_loaders import DirectoryLoader, TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from .config import config


class RagEngine:
    def __init__(self):
        self.vector_db: Optional[Chroma] = None
        self.retriever = None
        self.llm: Optional[ChatOpenAI] = None
        self.embeddings: Optional[HuggingFaceEmbeddings] = None

        # å»¶è¿Ÿåˆå§‹åŒ–ï¼šé¿å… bot å¯åŠ¨æ—¶å°±åŠ è½½ embedding/LLM
        # åªæœ‰åœ¨ /é—® æˆ– /é‡æ„çŸ¥è¯†åº“ æ—¶æ‰åˆå§‹åŒ–ã€‚

    def _ensure_initialized(self) -> None:
        if config.HF_ENDPOINT:
            os.environ["HF_ENDPOINT"] = config.HF_ENDPOINT

        if self.llm is None:
            if not config.AI_API_KEY:
                raise RuntimeError("æœªé…ç½® HITSZ_MANAGER_AI_API_KEY")
            self.llm = ChatOpenAI(
                openai_api_key=config.AI_API_KEY,
                openai_api_base=config.AI_BASE_URL,
                model_name=config.AI_MODEL,
                temperature=0.3,
            )

        if self.embeddings is None:
            print("ğŸ§  æ­£åœ¨åŠ è½½ Embedding æ¨¡å‹ (CPU)...")
            self.embeddings = HuggingFaceEmbeddings(
                model_name=config.EMBEDDING_MODEL,
                model_kwargs={"device": "cpu"},
                encode_kwargs={"normalize_embeddings": True},
            )

        self._load_existing_db()

    def _load_existing_db(self) -> None:
        if self.embeddings is None:
            return
        if config.VECTOR_DB_DIR.exists() and any(config.VECTOR_DB_DIR.iterdir()):
            try:
                self.vector_db = Chroma(
                    persist_directory=str(config.VECTOR_DB_DIR), 
                    embedding_function=self.embeddings
                )
                self.retriever = self.vector_db.as_retriever(search_kwargs={"k": 3})
                print("ğŸ“š æœ¬åœ°å‘é‡çŸ¥è¯†åº“åŠ è½½æˆåŠŸ")
            except Exception as e:
                print(f"âš ï¸ å‘é‡åº“åŠ è½½å¤±è´¥ (å¯èƒ½æ˜¯é¦–æ¬¡è¿è¡Œ): {e}")

    async def rebuild_index(self) -> str:
        """é‡å»ºçŸ¥è¯†åº“ç´¢å¼• (è€—æ—¶æ“ä½œ)"""
        try:
            self._ensure_initialized()
        except Exception as e:
            return f"âŒ åˆå§‹åŒ–å¤±è´¥: {e}"

        if not config.RAG_DOCS_DIR.exists():
            return "âŒ ç›®å½• data/rag_docs ä¸å­˜åœ¨"
        
        # 1. è¯»å–æ–‡ä»¶
        loader = DirectoryLoader(str(config.RAG_DOCS_DIR), glob="**/*.txt", loader_cls=TextLoader)
        docs = loader.load()
        if not docs:
            return "âš ï¸ data/rag_docs ç›®å½•ä¸‹æ²¡æœ‰ .txt æ–‡ä»¶"

        # 2. åˆ‡åˆ†æ–‡æœ¬
        splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)
        splits = splitter.split_documents(docs)

        # 3. å†™å…¥ Chroma
        # æ³¨æ„ï¼šè¿™é‡Œä¼šé‡æ–°ç”Ÿæˆæ•´ä¸ªåº“
        try:
            if self.vector_db:
                # å°è¯•æ¸…ç†æ—§æ•°æ®ï¼Œæˆ–è€…ç›´æ¥è¦†ç›–ç›®å½•
                self.vector_db = None
            
            # åˆ›å»ºæ–°çš„ DB
            self.vector_db = Chroma.from_documents(
                documents=splits,
                embedding=self.embeddings,
                persist_directory=str(config.VECTOR_DB_DIR)
            )
            self.retriever = self.vector_db.as_retriever(search_kwargs={"k": 3})
            return f"âœ… çŸ¥è¯†åº“æ„å»ºå®Œæˆï¼å…±ç´¢å¼• {len(splits)} ä¸ªæ–‡æœ¬ç‰‡æ®µã€‚"
        except Exception as e:
            return f"âŒ æ„å»ºå¤±è´¥: {e}"

    async def query(self, question: str) -> str:
        """RAG é—®ç­”æµç¨‹"""
        try:
            self._ensure_initialized()
        except Exception as e:
            return f"âŒ åˆå§‹åŒ–å¤±è´¥: {e}"

        if not self.retriever:
            return "âš ï¸ çŸ¥è¯†åº“å°šæœªåˆå§‹åŒ–ï¼Œè¯·å…ˆä½¿ç”¨æŒ‡ä»¤æ„å»ºçŸ¥è¯†åº“ã€‚"

        template = """ä½ æ˜¯ä¸€ä¸ªå“ˆå·¥å¤§æ·±åœ³(HITSZ)çš„æ ¡å›­åŠ©æ‰‹ã€‚è¯·æ ¹æ®ä»¥ä¸‹å·²çŸ¥ä¿¡æ¯å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚
        
        ä¸¥æ ¼éµå®ˆä»¥ä¸‹è§„åˆ™ï¼š
        1. ä»…æ ¹æ®[å·²çŸ¥ä¿¡æ¯]å›ç­”ï¼Œä¸è¦ç¼–é€ å†…å®¹ã€‚
        2. å¦‚æœå·²çŸ¥ä¿¡æ¯ä¸­æ²¡æœ‰ç­”æ¡ˆï¼Œè¯·ç›´æ¥å›ç­”â€œæŠ±æ­‰ï¼ŒçŸ¥è¯†åº“ä¸­æš‚æ—¶æ²¡æœ‰ç›¸å…³ä¿¡æ¯â€ã€‚
        3. å›ç­”è¦ç®€æ´æ˜äº†ã€‚

        [å·²çŸ¥ä¿¡æ¯]:
        {context}

        [ç”¨æˆ·é—®é¢˜]: {question}
        """
        prompt = ChatPromptTemplate.from_template(template)

        chain = (
            {"context": self.retriever, "question": RunnablePassthrough()}
            | prompt
            | self.llm
            | StrOutputParser()
        )

        try:
            return await chain.ainvoke(question)
        except Exception as e:
            return f"âŒ AI å‘ç”Ÿé”™è¯¯: {e}"

# å…¨å±€å•ä¾‹
rag_engine = RagEngine()
